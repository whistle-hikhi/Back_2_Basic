# Micrograd from Andrej Karpathy

Main point from this lecture is:
- Backpropagation
- Neuron, Layer, MLP
- Gradient, Derivative
- Batching: When deal with large amount of data we sampling randomly a small subset of data to put in training model at a time

It helps me to understand some pytorch code more deeply